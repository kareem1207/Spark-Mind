{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2dd8402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.5 environment at: D:\\python\\SparkMind\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m27 packages\u001b[0m \u001b[2min 1.46s\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m sentencepiece \u001b[2m(1.0MiB)\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m sentencepiece\n",
      "\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 1.51s\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 103ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.10.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msentencepiece\u001b[0m\u001b[2m==0.2.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%uv pip install transformers safetensors huggingface-hub accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d8925c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ForeKnow\\backend\\SparkMind\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ...existing code...\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch, shutil, os\n",
    "\n",
    "model_name = \"tabularisai/multilingual-sentiment-analysis\"\n",
    "cache_dir = r\"D:\\Models\\Sentiment\"\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "        mdl = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, cache_dir=cache_dir, torch_dtype=torch.float32\n",
    "        )\n",
    "        return tok, mdl\n",
    "    except OSError:\n",
    "        # Remove possibly corrupted snapshot and force re-download\n",
    "        snapshot_root = os.path.join(cache_dir, \"models--\" + model_name.replace(\"/\", \"--\"))\n",
    "        if os.path.isdir(snapshot_root):\n",
    "            shutil.rmtree(snapshot_root, ignore_errors=True)\n",
    "        tok = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir, force_download=True)\n",
    "        mdl = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, cache_dir=cache_dir, force_download=True, torch_dtype=torch.float32\n",
    "        )\n",
    "        return tok, mdl\n",
    "\n",
    "tokenizer, model = load_model_and_tokenizer()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3aafb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive [[0.0428469218313694, 0.16647836565971375, 0.1968902051448822, 0.5565119981765747, 0.037272531539201736]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "text = \"I… I woke up late today. Um… then I went to the kitchen...\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "pred = torch.softmax(outputs.logits, dim=1)\n",
    "labels = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
    "\n",
    "print(labels[torch.argmax(pred)], pred.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5c4a751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted sentiment score: 67.578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karee\\AppData\\Local\\Temp\\ipykernel_16604\\1380080934.py:2: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  weighted_sum: float = sum(p * w for p, w in zip(pred[0], label_wts))*100\n"
     ]
    }
   ],
   "source": [
    "label_wts : [float] =[0.2,0.4,0.6,0.8,1.0]\n",
    "weighted_sum: float = sum(p * w for p, w in zip(pred[0], label_wts))*100\n",
    "print(f\"Weighted sentiment score: {weighted_sum:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e37d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
